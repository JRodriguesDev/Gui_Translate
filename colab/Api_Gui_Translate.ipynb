{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL7Tpi0erHbe"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch accelerate bitsandbytes huggingface_hub fastapi uvicorn python-multipart\n",
        "!pip install ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkAebJAJsRXS"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSeq2SeqLM\n",
        "import os\n",
        "from ngrok import ngrok\n",
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "from pydantic import BaseModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zmnh1_1Frva7"
      },
      "outputs": [],
      "source": [
        "login('HUGGING FACE LOGIN')\n",
        "ngrok.set_auth_token('NGROK AUT TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkkvYCwTsOLS"
      },
      "outputs": [],
      "source": [
        "class TranslateModel:\n",
        "  def __init__(self):\n",
        "    self.model_id = 'google/madlad400-3b-mt'\n",
        "    self.model = None\n",
        "    self.tokenizer = None\n",
        "    self.bits_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                          bnb_4bit_quant_type=\"nf4\",\n",
        "                                          bnb_4bit_use_double_quant=True,\n",
        "                                          bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                                          llm_int8_threshold=6.0,\n",
        "                                          llm_int8_enable_fp32_cpu_offload=True)\n",
        "\n",
        "  def init_llm(self):\n",
        "    self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_id, quantization_config=self.bits_config, device_map='auto', trust_remote_code=True)\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, trust_remote_code=True)\n",
        "\n",
        "  def translate(self, text, lang):\n",
        "    prompt = f'<2{lang}> {text}'\n",
        "    input_ids = self.tokenizer(prompt, return_tensors='pt').input_ids.to(self.model.device)\n",
        "    outputs = self.model.generate(input_ids, max_new_tokens=512)\n",
        "    return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Jxg71DWuF9R"
      },
      "outputs": [],
      "source": [
        "app = FastAPI()\n",
        "translate_model = TranslateModel()\n",
        "translate_model.init_llm()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N64Qh8L8v5VD"
      },
      "outputs": [],
      "source": [
        "class TranslationRequest(BaseModel):\n",
        "  items: list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5yORL2l0s2u"
      },
      "outputs": [],
      "source": [
        "@app.get('/')\n",
        "async def init_api():\n",
        "  return {'message': 'Api Ativa'}\n",
        "\n",
        "@app.post('/translate')\n",
        "async def translate_endpoint(request: TranslationRequest):\n",
        "  translated_list = []\n",
        "  for items in request.items:\n",
        "    translated_text = translate_model.translate(items['text'], items['lang'])\n",
        "    translated_list.append(translated_text)\n",
        "  return {'translations': translated_list}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yvVXyTm415lO"
      },
      "outputs": [],
      "source": [
        "nest_asyncio.apply()\n",
        "async def setup_ngrok():\n",
        "  tunnel = await ngrok.connect(9002)\n",
        "  return tunnel\n",
        "\n",
        "async def run_server():\n",
        "  config = uvicorn.Config(app, host='0.0.0.0', port=9002)\n",
        "  server = uvicorn.Server(config)\n",
        "  await server.serve()\n",
        "\n",
        "asyncio.run(asyncio.gather(setup_ngrok(), run_server()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}